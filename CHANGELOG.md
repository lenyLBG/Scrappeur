# CHANGELOG

## Version 2.0.0 - Scraping R√©cursif (2025-12-17)

### ‚ú® Nouvelles Fonctionnalit√©s

#### 1. **Scraping R√©cursif** üîÑ
- **Nouvelle m√©thode `scrape_recursive()`**: Permet de scraper automatiquement plusieurs pages en suivant les liens
- **Param√®tre `max_depth`**: Contr√¥le la profondeur de r√©cursion
  - `0` = page actuelle seulement
  - `1` = page actuelle + pages li√©es
  - `2` = et ainsi de suite...
- **Param√®tre `allowed_domains`**: Limite le scraping √† certains domaines pour √©viter de crawler tout le web
- **Param√®tre `delay_between_requests`**: Ajoute un d√©lai respectueux entre les requ√™tes
- **Suivi automatique des liens**: Extraction et traitement r√©cursif des liens trouv√©s

#### 2. **Nouvelles M√©thodes**
- `_extract_links(url, allowed_domains)`: Extrait tous les liens d'une page avec filtrage par domaine
- Gestion des URLs relatives et absolues
- Stockage des pages visit√©es pour √©viter les doublons

#### 3. **Interface Am√©lior√©e**
- Menu interactif avec choix entre mode simple et mode r√©cursif
- Param√®tres configurables pour le scraping r√©cursif:
  - Profondeur maximale
  - D√©lai entre requ√™tes
- Affichage d√©taill√© de la progression

#### 4. **Documentation Enrichie**
- README.md mis √† jour avec exemples de scraping r√©cursif
- Nouveau fichier `example_recursive_scraping.py` avec 6 exemples pratiques
- Documentation compl√®te des param√®tres

### üîß Am√©liorations Techniques

- Import centralis√© de `urllib.parse` pour une meilleure gestion des URLs
- Nouvelles variables d'instance: `visited_urls`, `found_links`
- Logging d√©taill√© de la progression du scraping r√©cursif
- Affichage du nombre de pages visit√©es et de flux trouv√©s

### üìù Exemples d'Utilisation

#### Mode Simple (comme avant)
```python
with VideoScraper(browser='chrome') as scraper:
    urls = scraper.scrape_page('https://example.com/video')
```

#### Mode R√©cursif (nouveau!)
```python
with VideoScraper(browser='chrome') as scraper:
    urls = scraper.scrape_recursive(
        start_url='https://example.com',
        max_depth=2,
        allowed_domains=['example.com'],
        delay_between_requests=2
    )
```

### üéØ Cas d'Usage

1. **Extraire toutes les vid√©os d'un site web**: Utilisez `max_depth=2` ou `3`
2. **Respecter les serveurs**: Utilisez `delay_between_requests=3` pour ajouter des d√©lais
3. **Rester dans le domaine**: Utilisez `allowed_domains=['example.com']`
4. **Scraping rapide**: Utilisez `headless=True` et `max_depth=1`

### üì¶ Compatibilit√©

- ‚úÖ Python 3.8+
- ‚úÖ Chrome, Firefox, Edge
- ‚úÖ Windows, Linux, macOS
- ‚úÖ R√©tro-compatible avec l'ancienne m√©thode `scrape_page()`

### üöÄ Performance

- Scraping r√©cursif optimis√© pour limiter la consommation de ressources
- D√©lais configurables pour respecter les serveurs
- Mode headless pour une ex√©cution plus rapide
- Stockage des URLs visit√©es pour √©viter les doublons

### üìã Fichiers Modifi√©s

- `video_scraper.py`: Ajout de la m√©thode `scrape_recursive()` et `_extract_links()`
- `README.md`: Documentation compl√®te du scraping r√©cursif
- `example_recursive_scraping.py`: Nouveau fichier avec 6 exemples pratiques

### ‚ö†Ô∏è Notes Importantes

- Le scraping r√©cursif peut √™tre gourmand en ressources et en temps
- Respectez toujours les conditions d'utilisation des sites
- Utilisez des d√©lais appropri√©s (`delay_between_requests`) pour ne pas surcharger les serveurs
- L'option `allowed_domains` est fortement recommand√©e pour √©viter de crawler involontairement d'autres sites

---

## Version 1.0.0 - Release Initial (2025)

### Fonctionnalit√©s Initiales
- Support Chrome, Firefox, Edge
- D√©tection de flux vid√©o (HLS, DASH, MP4, etc.)
- Mode headless
- Analyse HTML
- Logs d√©taill√©s
- Export des r√©sultats
